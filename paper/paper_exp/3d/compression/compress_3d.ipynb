{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "2023-10-16 10:18:47.611721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import modules:\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "from aicsimageio import AICSImage\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import monai\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from monai.transforms import (\n",
    "    RandSpatialCropSamples,\n",
    "    LoadImage,\n",
    "    SaveImage,\n",
    "    Compose,\n",
    "    AddChannel,\n",
    "    RepeatChannel,\n",
    "    ToTensor,\n",
    "    Transform,\n",
    "    Transpose,\n",
    "    CastToType,\n",
    "    EnsureType,\n",
    "    ScaleIntensityRangePercentiles,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from compressai.zoo import image_models, models\n",
    "from compressai.zoo.pretrained import load_pretrained\n",
    "from aicsimageio import AICSImage\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "from tqdm.contrib import tenumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# define funcs and hyper-parameters:\n",
    "\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(_SEED)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def normalizeItensity(image):\n",
    "    # Convert the image data to a floating-point data type\n",
    "    img = image.astype(np.float32)\n",
    "    # Convert the image to a toch Tensor\n",
    "    img = torch.from_numpy(img)\n",
    "    # Normalize the intensity of the image using the MONAI NormalizeIntensity transform\n",
    "    normalize_intensity = monai.transforms.NormalizeIntensity()\n",
    "    img_normalized = normalize_intensity(img)\n",
    "    # Convert the normalized image back to a numpy array\n",
    "    return img_normalized.numpy()\n",
    "\n",
    "\n",
    "def transform_img(image):\n",
    "    img = image.astype(np.float32) / 65535\n",
    "    return img\n",
    "\n",
    "\n",
    "def compare_images(path1, path2, gt=True):\n",
    "    # Load the two images\n",
    "    image1 = AICSImage(path1).get_image_data(\"ZYX\")\n",
    "    image2 = AICSImage(path2).get_image_data(\"ZYX\")\n",
    "    # scale to 0-1\n",
    "    scaler = ScaleIntensity()\n",
    "    image1 = scaler(image1).cpu().numpy()\n",
    "    image2 = scaler(image2).cpu().numpy()\n",
    "    mse = np.sum((image1 - image2) ** 2) / (924 * 624)\n",
    "    ssim_value = ssim(image1, image2, data_range=1)\n",
    "    psnr_value = psnr(image1, image2, data_range=1)\n",
    "    corr = np.corrcoef(image1.ravel(), image2.ravel())[0, 1]\n",
    "    # psnr = 10 * np.log10(1 / (mse + 0.000001))\n",
    "    return mse, ssim_value, psnr_value, corr\n",
    "\n",
    "\n",
    "class Normalize(Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Rescale unint16 values to [0,1]\n",
    "        result = img / 65535.0\n",
    "        return result\n",
    "\n",
    "\n",
    "def torch2img(x: torch.Tensor):\n",
    "    # Convert  tensor to numpy array and rescale to uint16\n",
    "    np_array = x.clamp_(0, 1).squeeze().cpu().detach().numpy()\n",
    "    return np_array\n",
    "    # return (np_array * (2**16 - 1)).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "model = \"bmshj2018-factorized_3d\"\n",
    "device = torch.device(\"cpu\")\n",
    "metric = \"ms-ssim\"\n",
    "quality = 8\n",
    "model_info = models[model]\n",
    "checkpoint = \"/mnt/eternus/users/Yu/project/data_compression/experiment/3d_adaptation/model/3d_adaptation_v2_best.pth.tar\"\n",
    "# transform = Compose([LoadImage(image_only=True),AddChannel(), Transpose(indices = (0,3,1,2)), Normalize(), RandSpatialCropSamples(roi_size = (64,256,256), num_samples = 1, random_size = False, random_center = False)])\n",
    "transform = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        Transpose(indices=(0, 3, 2, 1)),\n",
    "        Normalize(),\n",
    "    ]\n",
    ")\n",
    "state_dict = torch.load(checkpoint, map_location=device)[\"state_dict\"]\n",
    "state_dict = load_pretrained(state_dict)\n",
    "net = (\n",
    "    model_info(quality=quality, metric=metric, pretrained=False)\n",
    "    .from_state_dict(state_dict)\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "net.update(force=True)\n",
    "\n",
    "# Global variable to store the call count\n",
    "call_count = 0\n",
    "\n",
    "\n",
    "# Custom decorator for counting function calls\n",
    "def counter(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global call_count  # Access the global variable\n",
    "        call_count += 1  # Increment the call count\n",
    "        result = func(*args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@counter\n",
    "def infer(img):\n",
    "    \"\"\"\n",
    "    img: (tensor) N x C x Z x H x W\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = net(img)[\"x_hat\"]\n",
    "        # compress_info = net.compress(img)\n",
    "\n",
    "    # Specify the file where you want to store the compress_info\n",
    "    # with open(\"compress_info.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(compress_info, file, protocol=5)  # Use Protocol 5\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "input_dir = sorted(\n",
    "    Path(\n",
    "        \"/mnt/eternus/users/Jianxu/projects/im2im_experiments_v1/data/labelfree3D/FBL/holdout\"\n",
    "    ).glob(\"*IM.tiff\")\n",
    ")\n",
    "output_dir = Path(\n",
    "    \"/mnt/eternus/users/Yu/project/data_compression/experiment/paper_exp/3d/compression/bmshj2018-factorized_ms-ssim_8_v2\"\n",
    ")\n",
    "compress_dir = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [1:14:06<00:00, 42.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, input in tenumerate(input_dir):\n",
    "    img = transform(input)[0].unsqueeze(0).unsqueeze(0).to(device)\n",
    "    # [img]->img, add batch channel, to device.\n",
    "    pred = sliding_window_inference(\n",
    "        inputs=img,\n",
    "        predictor=infer,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        roi_size=[32, 256, 256],\n",
    "        sw_batch_size=4,\n",
    "        overlap=0.1,\n",
    "        mode=\"gaussian\",\n",
    "    )\n",
    "    pred = torch2img(pred)\n",
    "    OmeTiffWriter.save(\n",
    "        pred,\n",
    "        (output_dir / f\"{input.stem}_pred{input.suffix}\"),\n",
    "        dim_order=\"ZYX\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [07:22,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.33440911084474484 SSIM: 0.9220190446920535 PSNR: 28.136645543964296 CORR: 0.9483616418217692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_paths = sorted((compress_dir).glob(\"*.tiff\"))\n",
    "mse_value = AverageMeter()\n",
    "ssim_value = AverageMeter()\n",
    "psnr_value = AverageMeter()\n",
    "corr_value = AverageMeter()\n",
    "for i, (decode_path, raw_path) in tenumerate(zip(decoded_paths, input_dir)):\n",
    "    tmp_mse, tmp_ssim, tmp_psnr, tmp_corr = compare_images(decode_path, raw_path)\n",
    "    mse_value.update(tmp_mse)\n",
    "    ssim_value.update(tmp_ssim)\n",
    "    psnr_value.update(tmp_psnr)\n",
    "    corr_value.update(tmp_corr)\n",
    "print(\n",
    "    \"MSE:\",\n",
    "    mse_value.avg,\n",
    "    \"SSIM:\",\n",
    "    ssim_value.avg,\n",
    "    \"PSNR:\",\n",
    "    psnr_value.avg,\n",
    "    \"CORR:\",\n",
    "    corr_value.avg,\n",
    ")\n",
    "# # Define the filename\n",
    "# filename = Path.cwd() / model_name / \"note.txt\"\n",
    "# # Open the file for writing (creating if it doesn't exist)\n",
    "# with open(filename, \"a\") as file:\n",
    "#     # Write the values to the file\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     file.write(f\"[Timestamp]: {timestamp}\\n\")\n",
    "#     file.write(f\"MSE: {mse_value.avg}\\n\")\n",
    "#     file.write(f\"SSIM: {ssim_value.avg}\\n\")\n",
    "#     file.write(f\"PSNR: {psnr_value.avg}\\n\")\n",
    "#     file.write(f\"Correlation: {corr_value.avg}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yz_compressAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
